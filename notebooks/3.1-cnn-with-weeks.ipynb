{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model (with weeks)\n",
    "\n",
    "This notebook trains a convolutional layer over the input probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import utils.data as udata\n",
    "import utils.dists as udists\n",
    "import utils.misc as u\n",
    "import os\n",
    "import losses\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from jrun import jin\n",
    "from tqdm import tqdm\n",
    "from keras.layers import (Activation, Convolution1D, Convolution2D, Dense,\n",
    "                          Dropout, Embedding, Flatten, Merge, Input)\n",
    "from keras.layers.merge import Concatenate, concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"collaborative\"\n",
    "data_dir = \"../data\"\n",
    "exp_dir = os.path.join(data_dir, \"processed\", EXP_NAME)\n",
    "\n",
    "with open(\"../config.yaml\") as fp:\n",
    "    CONFIG = yaml.load(fp)\n",
    "    \n",
    "TEST_SPLIT_THRESH = CONFIG[\"TEST_SPLIT_THRESH\"][EXP_NAME]\n",
    "\n",
    "COMPONENTS = [udata.Component(exp_dir, name) for name in u.available_models(exp_dir)]\n",
    "ACTUAL_DL = udata.ActualDataLoader(data_dir)\n",
    "\n",
    "REGIONS = [\"nat\", *[f\"hhs{i}\" for i in range(1, 11)], None]\n",
    "TARGETS = [udata.Target(t) for t in [1, 2, 3, 4, \"peak\", \"peak_wk\", \"onset_wk\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split\n",
    "\n",
    "We need to take the common row entries (common \"epiweek\", \"region\") for each data item, i.e. actual data and component data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = TARGETS[0]\n",
    "\n",
    "y_train, Xs_train, yi_train = target.get_training_data(\n",
    "    ACTUAL_DL, COMPONENTS, None, TEST_SPLIT_THRESH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating input matrices for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_train = u.encode_epiweeks_sin(np.array(yi_train[:, 0], dtype=np.int))\n",
    "X_train = udists.get_2d_features(Xs_train)\n",
    "y_one_hot_train = udists.actual_to_one_hot(y_train, bins=target.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1D_distribution(n_components,\n",
    "                        n_bins,\n",
    "                        week_embedding_size):\n",
    "    \"\"\"\n",
    "    One dimensional conv model over input distribution to give an output\n",
    "    distribution\n",
    "\n",
    "    Merges two branches\n",
    "    - predictions : (batch_size, n_bins, n_models)\n",
    "    - weeks : (batch_size, week_embedding_size)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int\n",
    "        Number of component distributions\n",
    "    n_bins : int\n",
    "        Number of bins in the prediction distribution\n",
    "    week_embedding_size : int\n",
    "        Embedding vector size for week\n",
    "    \"\"\"\n",
    "\n",
    "    dist_input = Input(shape=(n_bins, n_components))\n",
    "    week_input = Input(shape=(week_embedding_size,))\n",
    "\n",
    "    # Dist branch\n",
    "    if n_bins < 50:\n",
    "        dist_x = Convolution1D(32, 5, padding=\"same\")(dist_input)\n",
    "        dist_x = Convolution1D(10, 3, padding=\"same\")(dist_x)\n",
    "    else:\n",
    "        dist_x = Convolution1D(10, 3, padding=\"same\")(dist_input)\n",
    "        dist_x = Convolution1D(10, 3, padding=\"same\")(dist_x)\n",
    "\n",
    "    dist_x = Flatten()(dist_x)\n",
    "    dist_x = Dense(10, activation=\"tanh\")(dist_x)\n",
    "\n",
    "    week_x = Dense(5, activation=\"tanh\")(week_input)\n",
    "\n",
    "    merged = concatenate([dist_x, week_x])\n",
    "    merged = Dense(20, activation=\"relu\")(merged)\n",
    "    merged = Dense(n_bins, activation=\"softmax\")(merged)\n",
    "    \n",
    "    model = Model(inputs=[dist_input, week_input], outputs=merged)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generator\n",
    "def gen_model():\n",
    "    return conv1D_distribution(X_train.shape[-1], X_train.shape[1], weeks_train.shape[1])\n",
    "\n",
    "def train_model(\n",
    "    model, train_data, val_data,\n",
    "    batch_size=64, epochs=100,\n",
    "    verbose=0\n",
    "):\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "    if val_data is not None:\n",
    "        callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2, mode=\"auto\")]\n",
    "    else:\n",
    "        callbacks = []\n",
    "\n",
    "    history = model.fit(train_data[0],\n",
    "                        train_data[1],\n",
    "                        batch_size=batch_size, epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=val_data,\n",
    "                        shuffle=False)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_metadata = u.cv_train_loso(\n",
    "    gen_model, train_model,\n",
    "    [X_train, weeks_train], y_one_hot_train, yi_train\n",
    ")\n",
    "u.cv_plot(cv_metadata)\n",
    "cv_report = u.cv_report(cv_metadata)\n",
    "cv_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gen_model()\n",
    "final_epochs = int(cv_report[\"epochs\"][-1])\n",
    "final_history = train_model(\n",
    "    model,\n",
    "    [[X_train, weeks_train], y_one_hot_train],\n",
    "    None, epochs=final_epochs)\n",
    "final_loss = final_history.history[\"loss\"][-1]\n",
    "plt.plot(final_history.history[\"loss\"])\n",
    "final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eval_df = {\n",
    "    \"region\": [],\n",
    "    \"score\": []\n",
    "}\n",
    "\n",
    "for region in tqdm(REGIONS):\n",
    "    y_test, Xs_test, yi_test = target.get_testing_data(\n",
    "        ACTUAL_DL, COMPONENTS, region, TEST_SPLIT_THRESH\n",
    "    )\n",
    "    \n",
    "    weeks_test = u.encode_epiweeks_sin(np.array(yi_test[:, 0], dtype=np.int))\n",
    "    X_test = udists.get_2d_features(Xs_test)\n",
    "    output = model.predict([X_test, weeks_test])\n",
    "    y_one_hot = udists.actual_to_one_hot(y_test, bins=target.bins)\n",
    "    \n",
    "    eval_df[\"region\"].append(region if region is not None else \"all\")\n",
    "    eval_df[\"score\"].append(losses.mean_cat_cross(y_one_hot, output))\n",
    "\n",
    "eval_df = pd.DataFrame(eval_df)\n",
    "\n",
    "# Save results\n",
    "output_dir = u.ensure_dir(f\"../results/{EXP_NAME}/{target.name}\")\n",
    "u.save_exp_summary(model, cv_report, {\n",
    "    \"loss\": final_loss,\n",
    "    \"epochs\": final_epochs    \n",
    "}, f\"{output_dir}/cnn_week_summary.txt\")\n",
    "eval_df.to_csv(f\"{output_dir}/cnn-week.csv\", index=False)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
