#+TITLE: Data

This is a documentation of data as present in this directory (~./data/~). The
general directory structure is shown below.

#+BEGIN_SRC bash :results output :exports both
tree -L 2 -a
#+END_SRC

#+RESULTS:
#+begin_example
.
├── external
│   ├── adaptively-weighted-ensemble
│   ├── ensemble-data.csv.gz
│   └── flusight-data-dir
├── processed
│   ├── actual.csv
│   └── components
└── README.org

5 directories, 3 files
#+end_example

The data pre processing workflow involves putting in the data sources in
appropriate ~./external~ directory and running corresponding snakemake task from
the repository root. After processing, the component model data are collected in
the directory ~./processed/components/~. The data from here is then picked up by
the notebooks.

* ~external~

This contains external sources for model prediction. There are two types of
sources:

** ~adaptively-weighted-ensemble~ 

This keeps ~.rds~ outputs from [[https://github.com/reichlab/adaptively-weighted-ensemble/][adaptively-weighted-ensemble]] repository. Running
the following snakemake task from the root of this repository will pull in
required data from this directory in a single ~./external/ensemble-data.csv.gz~.

#+BEGIN_SRC bash
snakemake assemble_ensemble_data
#+END_SRC

The generated csv.gz file is version controlled in the ~./external/~ directory.
After generating the csv.gz, running the following command generate separate
model prediction files in ~./processed/components/~.

#+BEGIN_SRC bash
snakemake separate_ensemble_data
#+END_SRC

** ~flusight-data-dir~

This keeps CDC format submissions files in flusight style directory structure. A
sample structure is shown (upto model directory level):

#+BEGIN_SRC bash :results output :exports both
tree -L 2 external/flusight-data-dir
#+END_SRC

#+RESULTS:
#+begin_example
external/flusight-data-dir
├── 2010-2011
│   ├── CU-RHF_SEIRS
│   ├── Delphi-EmpiricalTraj
│   └── LANL-DBM
├── 2011-2012
│   ├── CU-RHF_SEIRS
│   ├── Delphi-EmpiricalTraj
│   └── LANL-DBM
├── 2012-2013
│   ├── CU-RHF_SEIRS
│   ├── Delphi-EmpiricalTraj
│   └── LANL-DBM
├── 2013-2014
│   ├── CU-RHF_SEIRS
│   ├── Delphi-EmpiricalTraj
│   └── LANL-DBM
├── 2014-2015
│   ├── CU-RHF_SEIRS
│   ├── Delphi-EmpiricalTraj
│   └── LANL-DBM
├── 2015-2016
│   ├── CU-RHF_SEIRS
│   ├── Delphi-EmpiricalTraj
│   └── LANL-DBM
└── 2016-2017
    ├── CU-RHF_SEIRS
    ├── Delphi-EmpiricalTraj
    └── LANL-DBM

28 directories, 0 files
#+end_example

These csvs can be processed using the following command from repo root:

#+BEGIN_SRC bash
snakemake pull_flusight_data
#+END_SRC

* ~processed~

This contains the final processed data which are then used by notebooks. For
each model, there is a directory inside ~./processed/components/~ containing the
following files:

#+BEGIN_SRC bash :export both :results output
ls -al ./processed/components/CU-BMA
#+END_SRC

#+RESULTS:
#+begin_example
total 4220
drwxr-xr-x  2 lepisma lepisma   4096 Oct 30 00:15 .
drwxrwxr-x 23 lepisma lepisma   4096 Oct 30 01:11 ..
-rw-r--r--  1 lepisma lepisma 838046 Oct 30 00:15 1.np.gz
-rw-r--r--  1 lepisma lepisma 874753 Oct 30 00:15 2.np.gz
-rw-r--r--  1 lepisma lepisma 916887 Oct 30 00:15 3.np.gz
-rw-r--r--  1 lepisma lepisma 938858 Oct 30 00:15 4.np.gz
-rw-r--r--  1 lepisma lepisma  30639 Oct 30 00:15 index.csv
-rw-r--r--  1 lepisma lepisma  78022 Oct 30 00:15 onset_wk.np.gz
-rw-r--r--  1 lepisma lepisma 489009 Oct 30 00:15 peak.np.gz
-rw-r--r--  1 lepisma lepisma 127921 Oct 30 00:15 peak_wk.np.gz
#+end_example

Each ~np.gz~ file is a compressed numpy matrix with predefined number of columns
for each bin and some number of rows mapped to actual epiweek and region using
the file ~index.csv~. A sample index file is shown below:

#+BEGIN_SRC bash :export both :results output
head ./processed/components/CU-BMA/index.csv
#+END_SRC

#+RESULTS:
#+begin_example
epiweek,region
201341,nat
201341,hhs1
201341,hhs2
201341,hhs3
201341,hhs4
201341,hhs5
201341,hhs6
201341,hhs7
201341,hhs8
#+end_example

The ~np.gz~ files have some fixed number of columns they contain depending on what
type of prediction they are. A detailed description follows:

#+BEGIN_SRC js
  // Number of columns for np.gz files
  {
    "1.np.gz": 131, // percent bins starting from [0.0, 0.1, ..., 13.0]
    "2.np.gz": 131,
    "3.np.gz": 131,
    "4.np.gz": 131,
    "onset_wk.np.gz": 34, // week bins starting from [40, 41, ..., 52/53, 1, 2, ..., 19/20], last one for 'none' bin
    "peak.np.gz": 131,
    "peak_wk.np.gz": 33 // week bins
  }
#+END_SRC

Consistency of these files can be checked by running the following test from the
root directory:

#+BEGIN_SRC bash :export both
pytest ./tests/test_components.py
#+END_SRC

This directory also contains ~./processed/actual.csv~ which has actual wili data
as shown below:

#+BEGIN_SRC bash :export both :results output
head ./processed/actual.csv
#+END_SRC

#+RESULTS:
#+begin_example
epiweek,region,wili
199740,nat,1.1014825590386
199741,nat,1.2000682166927
199742,nat,1.378763290902
199743,nat,1.1991993499089
199744,nat,1.656177824396
199745,nat,1.4132646790523
199746,nat,1.9867965365904
199747,nat,2.4474939169092
199748,nat,1.7390086933452
#+end_example

The actual data can be downloaded using the [[https://github.com/cmu-delphi/delphi-epidata][delphi-epidata]] api by running the
following from the repo root:

#+BEGIN_SRC bash :export both
snakemake get_actual_data
#+END_SRC
