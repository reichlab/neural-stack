#+TITLE: Neural Stack Writeup
#+OPTIONS: author:nil

#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{biblatex}
#+LATEX_HEADER: \addbibresource{~/library.bib}

* Structure

** Introduction
What did you do? Why?

** Methods
In this section, we describe the dataset used, experimental setup and details of
models evaluated.

[ /Dataset description like earlier paper(s)/ ]

Unlike previous work, we don't rely on the performance of component models for
estimating a set of /weights/. Instead, we use the predictions (probability
distributions) from the components as input to the ensemble model and predict
directly the output for the wILI prediction problem, skipping any intermediate
weight estimation.

*** Component model inputs / outputs
[ /Something similar to earlier papers/ ]

*** Ensemble model inputs
The ensemble models /merge/ probability distributions from input component
models and predict probability distributions, in the same space, as output.
During training, the mean categorical crossentropy loss between the output
distribution and the actual wILI value, as one-hot distribution, is minimized.
This loss minimization is equivalent to maximizing the log score as described by
CDC [ /Elaborate more on the CDC loss/ ].

Other than the probability distributions, the ensemble models also have
information about the current week (the week they are merging probabilities for)
of the season. Since week goes from 1 to 52/53, to preserve continuity, we
encode each week ($w$) in a vector $\bar{w}$ of two elements in a sinusoidal
representation as follows:

\[ \bar{w} = [\sin(\frac{2 \pi w}{n}), \cos(\frac{2 \pi w}{n})] \]

Where $n$ is the total number of weeks (52/53) in the season year. For each
neural network model, we create a /with-week/ variant which takes in $\bar{w}$
as one of its input.

*** Models
We evaluate two neural network models for the stacking task. The first model
(mixture density network) works by approximating the input probability
distributions using a gaussian and the output as a mixture of gaussians. The
second model (convolutional neural network) works directly on the probability
distributions (as vector of bin values) from components as input and returns a
vector of values as probability distribution as output. A general description of
neural network follows:

**** *Neural Networks*

Neural networks (or Artificial Neural networks) are machine learning tools
modelled loosely after the way neurons are connected in animal brains. The high
level aim is to learn a mapping from input to output which may be non-linear. In
a neural network, the /neurons/ are arranged in some number of /hidden layers/ along
with an input and an output layer [ TODO: Add figure ].

In the most general case of a feedforward neural network, the neurons in i^{th}
layer have incoming connections from all the neurons in (i - 1)^{th} layer and
outgoing connections to all the neurons in (i + 1)^{th} layer. Each neuron in
itself collects /its/ input values (also called activations of the input neurons),
uses its personal set of weights to find a weighted sum of them and passes the
result through an activation function to produce its activation value. [ TODO:
Add figure ]. The whole pipeline effectively results in a mapping from input to
output parametrized by the neuron connection weights.

To actually fit a model for the input and output, the network needs to train its
weights so that it minimizes a certain loss function. The loss function is
problem dependent and describes how poorly the output of the network matches
with the actual output for the same input. This training is done using
backpropagation which is a simple application of chain rule of differentiation
for propagating the gradient of loss function to all the neurons so that their
weights may be nudged in the right direction for reaching lower loss values. [
TODO: Need more explaining on SGD? ]

Neural networks have been successful on a variety of tasks [ TODO: Cite ].
Recent advancements in the techniques and tooling have made it possible to train
very /deep/ networks capable of learning highly non-linear mappings with high
generalization. A short review of these /deep learning/ methods is presented in
cite:lecun2015deep.

**** *Mixture density network*

A mixture density network cite:bishop1994mixture is a simple feed forward neural
network which outputs parameters for a mixture of distributions. The loss
function here is the crossentropy loss between the mixture of distributions
generated by the network and one-hot representation of the truth.

The model we use /assumes/ the output from the component models as normally
distributed with certain mean and standard deviation. This translates to
assuming a single gaussian peak in the output probability distribution from the
inputs. It takes in these two inputs (mean and standard deviation of the
distribution) from each of the component models and returns a mixture of $n$
gaussians by outputting a set of means ($\mu_i$), standard deviations
($\sigma_i$) and weights ($w_i$) for each distribution in the mixture. The final
distribution for a network outputting $n$ mixtures is then given by:

\begin{equation}
F(x) = \sum_{i = 1}^{n} w_i f(x, \mu_i, \sigma_i^2)
\end{equation}

Where $f(x, \mu_i, \sigma_i^2)$ represents a gaussian with mean $\mu_i$ and
variance $\sigma_i^2$. Figure [@fig:mdn] shows the structure of a mixture
density model (with weeks) [ /REFER TO GITHUB/ ]

#+CAPTION: Graph of the mixture density network model. This specific network takes
#+CAPTION: in means and standard deviations of 21 component models (42 inputs) and 2 inputs
#+CAPTION: encoding week. It outputs 6 parameters to be interpreted as weights, means
#+CAPTION: and standard deviations for a mixture of 2 gaussians.
#+LABEL: fig:mdn
#+ATTR_LATEX: :width 7cm :options
[[./images/mdn_model.pdf]]

**** *Convolutional neural network*

A convolutional neural network (CNN) [ TODO: cite and refer ] are neural
networks characterized generally by presence of convolutional layers. These
layers differ from the regular fully connected layers in that the neurons here
are only connected to /local/ patches in previous layers. Each convolutional
layer has a set of /locally responsive/ filters [ TODO: image and connected
description ].

The CNN model in our work puts less assumptions on the input and output
distributions and uses a set of 1-dimensional convolutional layers over the
complete discrete input distributions. As the output, it outputs the complete
discrete probability distribution vector. Figure [@fig:cnn] shows the structure
of a convolutional model with weeks [ TODO: /REFER TO GITHUB/ ]

#+CAPTION: Graph of a convolutional neural model for wili target. The input on the
#+CAPTION: left branch is a set of probability distributions (130 bins) representing
#+CAPTION: wili values for 21 component models. The right branch takes in encoded weeks
#+CAPTION: as vector of size 2. The model finally outputs a probability distribution
#+CAPTION: using 130 bins (same as the component models).
#+LABEL: fig:cnn
#+ATTR_LATEX: :width 10cm :options
[[./images/cnn_model.pdf]]

** Results
What did you find?
** Discussion
What does it all mean?
** Conclusions

\printbibliography{}
